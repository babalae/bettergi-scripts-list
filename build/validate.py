#!/usr/bin/python
# -*- coding: utf-8 -*-
import json
import os
import subprocess
import re
import chardet
from packaging.version import parse
from semver import VersionInfo

# ==================== é…ç½®å’Œå¸¸é‡ ====================

# å®šä¹‰æœ‰æ•ˆçš„ type å’Œ move_mode å€¼
VALID_TYPES = ["teleport", "path", "target", "orientation"]
VALID_MOVE_MODES = ["swim", "walk", "fly", "climb", "run", "dash", "jump"]

# å®šä¹‰ action å’Œ action_params çš„æœ€ä½å…¼å®¹ç‰ˆæœ¬
ACTION_VERSION_MAP = {
    "fight": "0.42.0",
    "mining": "0.43.0",
    "fishing": "0.43.0",
    "force_tp": "0.42.0",
    "log_output": "0.42.0",
    "anemo_collect": "0.42.0",
    "combat_script": "0.42.0",
    "hydro_collect": "0.42.0",
    "pick_around": "0.42.0",
    "pyro_collect": "0.43.0",
    "stop_flying": "0.42.0",
    "normal_attack": "0.42.0",
    "electro_collect": "0.42.0",
    "nahida_collect": "0.42.0",
    "up_down_grab_leaf": "0.42.0",
    "set_time": "0.45.0",
    "exit_and_relogin": "0.46.0",
    "use_gadget": "0.48.1"
}

# å®šä¹‰ action_params çš„æœ€ä½å…¼å®¹ç‰ˆæœ¬å’Œæ­£åˆ™è¡¨è¾¾å¼éªŒè¯
ACTION_PARAMS_VERSION_MAP = {
    "stop_flying": {
        "params": {"version": "0.44.0", "regex": r"^\d+(\.\d+)?$"}
    },
    "pick_around": {
        "params": {"version": "0.42.0", "regex": r"^\d+$"}
    },
    "combat_script": {
        "params": {"version": "0.42.0", "regex": r"^.+$"}  # ä»»æ„éç©ºå­—ç¬¦ä¸²
    },
    "log_output": {
        "params": {"version": "0.42.0", "regex": r"^.+$"}  # ä»»æ„éç©ºå­—ç¬¦ä¸²
    }
    # å…¶ä»– action ç±»å‹æ²¡æœ‰æ˜ç¡®çš„ action_params æ ¼å¼è¦æ±‚
}

# é»˜è®¤ç‰ˆæœ¬å·
DEFAULT_BGI_VERSION = "0.52.0"
DEFAULT_VERSION = "1.0"

# ==================== æ–‡ä»¶æ“ä½œ ====================

def get_original_file(file_path):
    """ä»ä¸Šæ¸¸ä»“åº“è·å–åŸå§‹æ–‡ä»¶å†…å®¹ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•ä»æœ¬åœ°è·å–"""
    # è¿”å›å€¼å¢åŠ ä¸€ä¸ªæ¥æºæ ‡è¯†: "upstream", "pr_submitted", None

    try:
        result = subprocess.run(['git', 'show', f'upstream/main:{file_path}'],
                                capture_output=True, text=True, encoding='utf-8')
        if result.returncode == 0:
            return json.loads(result.stdout), "upstream"
    except Exception as e:
        print(f"ä»ä¸Šæ¸¸ä»“åº“è·å–åŸå§‹æ–‡ä»¶å¤±è´¥: {str(e)}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            current_data = json.load(f)
            # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ï¼Œé¿å…å¼•ç”¨ç›¸åŒçš„å¯¹è±¡
            return json.loads(json.dumps(current_data)), "pr_submitted"
    except Exception as e:
        print(f"è¯»å–å½“å‰æ–‡ä»¶å¤±è´¥: {str(e)}")

    return None, None

def load_json_file(file_path):
    """åŠ è½½ JSON æ–‡ä»¶"""
    try:
        with open(file_path, encoding='utf-8') as f:
            return json.load(f), None
    except Exception as e:
        return None, f"âŒ JSON æ ¼å¼é”™è¯¯: {str(e)}"

def save_json_file(file_path, data):
    """ä¿å­˜ JSON æ–‡ä»¶"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False

# ==================== ç‰ˆæœ¬å¤„ç† ====================

def process_version(current, original, is_new):
    """å¤„ç†ç‰ˆæœ¬å·æ›´æ–°é€»è¾‘"""
    if is_new:
        return DEFAULT_VERSION

    if not original:
        return DEFAULT_VERSION

    try:
        cv = parse(current)
        ov = parse(original)
        # å¼ºåˆ¶æ›´æ–°ç‰ˆæœ¬å·ï¼Œæ— è®ºå½“å‰ç‰ˆæœ¬æ˜¯å¦å¤§äºåŸå§‹ç‰ˆæœ¬
        return f"{ov.major}.{ov.minor + 1}"
    except Exception:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„æ•°å­—å¤„ç†
        parts = original.split('.')
        if len(parts) >= 2:
            try:
                major = int(parts[0])
                minor = int(parts[1])
                return f"{major}.{minor + 1}"
            except ValueError:
                pass
        return f"{original}.1"

def extract_required_version(compatibility_issues):
    """ä»å…¼å®¹æ€§é—®é¢˜ä¸­æå–æ‰€éœ€çš„æœ€é«˜ç‰ˆæœ¬å·"""
    required_versions = []
    for issue in compatibility_issues:
        parts = issue.split(">=")
        if len(parts) > 1:
            version_part = parts[1].split(",")[0].strip()
            version_match = re.search(r'(\d+\.\d+\.\d+)', version_part)
            if version_match:
                required_versions.append(version_match.group(1))

    if not required_versions:
        return None

    try:
        return max(required_versions, key=lambda v: VersionInfo.parse(v))
    except ValueError:
        return None

def parse_bgi_version(version_str):
    """è§£æ BGI ç‰ˆæœ¬å·"""
    try:
        # ç¡®ä¿åˆ é™¤ v å‰ç¼€
        return VersionInfo.parse(version_str.lstrip('v'))
    except ValueError:
        return None

# ==================== å­—æ®µéªŒè¯ ====================

def check_action_compatibility(action_type, action_params, bgi_version):
    """æ£€æŸ¥ action å’Œ action_params ä¸ BGI ç‰ˆæœ¬çš„å…¼å®¹æ€§"""
    issues = []
    validation_issues = []

    # å¦‚æœ action_type ä¸ºç©ºï¼Œåˆ™è·³è¿‡æ£€æŸ¥
    if not action_type:
        return issues, validation_issues

    # ç¡®ä¿ bgi_version æ˜¯æœ‰æ•ˆçš„æ ¼å¼
    bgi_ver = parse_bgi_version(bgi_version)
    if not bgi_ver:
        validation_issues.append(f"æ— æ•ˆçš„ bgi_version æ ¼å¼: {bgi_version}")
        return issues, validation_issues

    # æ£€æŸ¥ action å…¼å®¹æ€§
    if action_type in ACTION_VERSION_MAP:
        min_version = ACTION_VERSION_MAP[action_type]
        try:
            if bgi_ver < VersionInfo.parse(min_version):
                issues.append(f"action '{action_type}' éœ€è¦ BGI ç‰ˆæœ¬ >= {min_version}ï¼Œå½“å‰ä¸º {bgi_version}")
        except ValueError:
            validation_issues.append(f"æ— æ³•æ¯”è¾ƒç‰ˆæœ¬: {min_version} ä¸ {bgi_version}")
    else:
        validation_issues.append(f"æœªçŸ¥çš„ action ç±»å‹: '{action_type}'ï¼Œå·²çŸ¥ç±»å‹: {', '.join(sorted(ACTION_VERSION_MAP.keys()))}")

    # æ£€æŸ¥ action_params å…¼å®¹æ€§å’Œæ ¼å¼
    if action_type in ACTION_PARAMS_VERSION_MAP and action_params:
        param_info = ACTION_PARAMS_VERSION_MAP[action_type]["params"]
        min_version = param_info["version"]
        regex_pattern = param_info["regex"]

        # ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
        try:
            if bgi_ver < VersionInfo.parse(min_version):
                issues.append(f"action '{action_type}' çš„å‚æ•°éœ€è¦ BGI ç‰ˆæœ¬ >= {min_version}ï¼Œå½“å‰ä¸º {bgi_version}")
        except ValueError:
            validation_issues.append(f"æ— æ³•æ¯”è¾ƒç‰ˆæœ¬: {min_version} ä¸ {bgi_version}")

        # å‚æ•°æ ¼å¼éªŒè¯
        if not re.match(regex_pattern, str(action_params)):
            validation_issues.append(f"action '{action_type}' çš„å‚æ•°æ ¼å¼ä¸æ­£ç¡®: '{action_params}'ï¼Œåº”åŒ¹é…æ¨¡å¼: {regex_pattern}")

    return issues, validation_issues

def process_coordinates(positions):
    """ç»Ÿä¸€å¤„ç†åæ ‡ä¿ç•™ä¸¤ä½å°æ•°é€»è¾‘"""
    coord_changed = False
    for pos in positions:
        for axis in ['x', 'y']:
            if axis in pos and isinstance(pos[axis], (int, float)):
                original = pos[axis]
                pos[axis] = round(float(pos[axis]), 4)
                if original != pos[axis]:
                    coord_changed = True
    return coord_changed

def ensure_required_fields(info, filename):
    """ç»Ÿä¸€å¤„ç†å¿…è¦å­—æ®µæ£€æŸ¥é€»è¾‘"""
    corrections = []

    if info["name"] != filename:
        info["name"] = filename
        corrections.append(f"name è‡ªåŠ¨ä¿®æ­£ä¸º {filename}")

    if info["type"] not in ["collect", "fight"]:
        info["type"] = "collect"
        corrections.append("type è‡ªåŠ¨ä¿®æ­£ä¸º collect")

    if not info["authors"]:
        author_name = os.getenv("GITHUB_ACTOR", "æœªçŸ¥ä½œè€…")
        author_link = "https://github.com/" + os.getenv("GITHUB_ACTOR", "babalae/bettergi-scripts-list")
        info["authors"] = [{"name": author_name, "links": author_link}]
        corrections.append(f"authors è‡ªåŠ¨è®¾ç½®ä¸º {info['authors']}")

    return corrections

def check_position_fields(positions):
    """æ£€æŸ¥ä½ç½®å­—æ®µçš„æœ‰æ•ˆæ€§

    è‡ªåŠ¨ä¿®å¤åŠŸèƒ½:
    1. ç¼ºå°‘ type å­—æ®µæ—¶ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º 'path'
    2. type å­—æ®µæ— æ•ˆæ—¶ï¼Œè‡ªåŠ¨ä¿®æ­£ä¸º 'path'
    3. å½“ type ä¸º 'path' æˆ– 'target' ä¸”ç¼ºå°‘ move_mode æ—¶ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º 'walk'
    4. move_mode å­—æ®µæ— æ•ˆæ—¶ï¼Œè‡ªåŠ¨ä¿®æ­£ä¸º 'walk'
    """
    validation_issues = []
    notices = []
    corrections = []  # æ·»åŠ ä¿®æ­£åˆ—è¡¨

    for idx, pos in enumerate(positions):
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["x", "y", "type"]
        missing_fields = [field for field in required_fields if field not in pos]

        if missing_fields:
            validation_issues.append(f"ä½ç½® {idx+1} ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
            # è‡ªåŠ¨æ·»åŠ ç¼ºå¤±çš„ type å­—æ®µ
            if "type" in missing_fields:
                pos["type"] = "path"  # è‡ªåŠ¨ä¿®å¤ï¼šç¼ºå°‘ type å­—æ®µæ—¶è®¾ç½®ä¸º path
                corrections.append(f"ä½ç½® {idx+1} ç¼ºå°‘ type å­—æ®µï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤å€¼ 'path'")
                # å¦‚æœæ·»åŠ äº† path ç±»å‹ï¼Œä¹Ÿéœ€è¦æ·»åŠ  move_mode
                if "move_mode" not in pos:
                    pos["move_mode"] = "walk"  # è‡ªåŠ¨ä¿®å¤ï¼šä¸º path ç±»å‹æ·»åŠ é»˜è®¤ move_mode
                    corrections.append(f"ä½ç½® {idx+1} ç¼ºå°‘ move_mode å­—æ®µï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤å€¼ 'walk'")
            # ç§»é™¤ continueï¼Œç¡®ä¿åç»­æ£€æŸ¥èƒ½å¤Ÿæ‰§è¡Œ
            # continue

        # éªŒè¯ type å­—æ®µ
        if "type" in pos:
            pos_type = pos["type"]
            if pos_type not in VALID_TYPES:
                validation_issues.append(f"ä½ç½® {idx+1}: type '{pos_type}' æ— æ•ˆï¼Œæœ‰æ•ˆå€¼ä¸º: {', '.join(VALID_TYPES)}")
                # è‡ªåŠ¨ä¿®æ­£æ— æ•ˆçš„ type å­—æ®µ
                pos["type"] = "path"  # è‡ªåŠ¨ä¿®å¤ï¼šæ— æ•ˆ type ä¿®æ­£ä¸º path
                corrections.append(f"ä½ç½® {idx+1} çš„ type '{pos_type}' æ— æ•ˆï¼Œå·²ä¿®æ­£ä¸º 'path'")
                pos_type = "path"  # æ›´æ–° pos_type ä»¥ä¾¿åç»­æ£€æŸ¥

            # å½“ type ä¸º path æˆ– target æ—¶ï¼ŒéªŒè¯ move_mode
            if pos_type in ["path", "target"]:
                if "move_mode" not in pos:
                    validation_issues.append(f"ä½ç½® {idx+1}: type ä¸º '{pos_type}' æ—¶å¿…é¡»æŒ‡å®š move_mode")
                    # è‡ªåŠ¨æ·»åŠ ç¼ºå¤±çš„ move_mode
                    pos["move_mode"] = "walk"  # è‡ªåŠ¨ä¿®å¤ï¼šç¼ºå°‘ move_mode æ—¶è®¾ç½®ä¸º walk
                    corrections.append(f"ä½ç½® {idx+1} ç¼ºå°‘ move_mode å­—æ®µï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤å€¼ 'walk'")
                elif pos["move_mode"] not in VALID_MOVE_MODES:
                    validation_issues.append(f"ä½ç½® {idx+1}: move_mode '{pos['move_mode']}' æ— æ•ˆï¼Œæœ‰æ•ˆå€¼ä¸º: {', '.join(VALID_MOVE_MODES)}")
                    # è‡ªåŠ¨ä¿®æ­£æ— æ•ˆçš„ move_mode
                    pos["move_mode"] = "walk"  # è‡ªåŠ¨ä¿®å¤ï¼šæ— æ•ˆ move_mode ä¿®æ­£ä¸º walk
                    corrections.append(f"ä½ç½® {idx+1} çš„ move_mode '{pos['move_mode']}' æ— æ•ˆï¼Œå·²ä¿®æ­£ä¸º 'walk'")

        # æ£€æŸ¥ç¬¬ä¸€ä¸ªä½ç½®æ˜¯å¦ä¸º teleport
        if idx == 0 and pos.get("type") != "teleport":
            notices.append("âš ï¸ ç¬¬ä¸€ä¸ª position çš„ type ä¸æ˜¯ teleport")

    return validation_issues, notices, corrections

def check_bgi_version_compatibility(bgi_version, auto_fix=False):
    """æ£€æŸ¥ BGI ç‰ˆæœ¬å…¼å®¹æ€§"""
    corrections = []

    # åˆ é™¤å¯èƒ½å­˜åœ¨çš„ v å‰ç¼€
    if bgi_version.startswith('v'):
        bgi_version = bgi_version.lstrip('v')
        corrections.append(f"bgi_version å‰ç¼€ 'v' å·²åˆ é™¤")

    bgi_ver = parse_bgi_version(bgi_version)

    if not bgi_ver:
        if auto_fix:
            corrections.append(f"bgi_version {bgi_version} æ ¼å¼æ— æ•ˆï¼Œè‡ªåŠ¨æ›´æ–°ä¸º {DEFAULT_BGI_VERSION}")
            return DEFAULT_BGI_VERSION, corrections
        return bgi_version, []

    if bgi_ver < VersionInfo.parse(DEFAULT_BGI_VERSION):
        if auto_fix:
            corrections.append(f"bgi_version {bgi_version} è‡ªåŠ¨æ›´æ–°ä¸º {DEFAULT_BGI_VERSION} (åŸç‰ˆæœ¬ä½äºè¦æ±‚)")
            return DEFAULT_BGI_VERSION, corrections

    return bgi_version, corrections

def check_position_ids(positions):
    """æ£€æŸ¥å¹¶ä¿®å¤ä½ç½® ID ç¼–ç¼–å·çš„è¿ç»­æ€§
    
    è‡ªåŠ¨ä¿®å¤åŠŸèƒ½:
    1. ç¼ºå°‘ id å­—æ®µæ—¶ï¼Œè‡ªåŠ¨æŒ‰é¡ºåºæ·»åŠ 
    2. id ç¼–å·ä¸è¿ç»­æ—¶ï¼Œè‡ªåŠ¨é‡æ–°æ’åº
    3. id ä¸æ˜¯ä» 1 å¼€å§‹æ—¶ï¼Œè‡ªåŠ¨è°ƒæ•´
    4. id å€¼æ— æ•ˆï¼ˆéæ•°å­—ï¼‰æ—¶ï¼Œè‡ªåŠ¨ä¿®æ­£
    """
    corrections = []
    validation_issues = []
    
    if not positions:
        return validation_issues, corrections
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä½ç½®éƒ½æœ‰ id å­—æ®µï¼Œå¹¶æ”¶é›†ç°æœ‰ id å€¼
    current_ids = []
    missing_ids = []
    invalid_ids = []
    
    for idx, pos in enumerate(positions):
        if "id" not in pos:
            missing_ids.append(idx)
            current_ids.append(None)
        else:
            try:
                id_val = int(pos["id"])
                current_ids.append(id_val)
            except (ValueError, TypeError):
                # å¦‚æœ id ä¸æ˜¯æ•°å­—ï¼Œè®°å½•ä¸ºæ— æ•ˆ
                invalid_ids.append(idx)
                current_ids.append(None)
    
    # å¦‚æœæœ‰ç¼ºå°‘ id çš„ä½ç½®ï¼Œè®°å½•
    if missing_ids:
        corrections.append(f"ä¸º {len(missing_ids)} ä¸ªä½ç½®è‡ªåŠ¨æ·»åŠ äº† id å­—æ®µ")
    
    # å¦‚æœæœ‰æ— æ•ˆ idï¼Œè®°å½•
    if invalid_ids:
        corrections.append(f"ä¿®æ­£äº† {len(invalid_ids)} ä¸ªæ— æ•ˆçš„ id å€¼")
    
    # ç”ŸæˆæœŸæœ›çš„ id åºåˆ—ï¼ˆä» 1 å¼€å§‹ï¼‰
    expected_ids = list(range(1, len(positions) + 1))
    
    # æ£€æŸ¥å½“å‰ id æ˜¯å¦ç¬¦åˆæœŸæœ›
    needs_reorder = False
    
    # è¿‡æ»¤æ‰ None å€¼æ¥æ£€æŸ¥ç°æœ‰çš„æœ‰æ•ˆ id
    valid_current_ids = [id_val for id_val in current_ids if id_val is not None]
    
    if len(valid_current_ids) != len(positions):
        needs_reorder = True
    elif valid_current_ids != expected_ids:
        needs_reorder = True
    else:
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„ id
        if len(set(valid_current_ids)) != len(valid_current_ids):
            needs_reorder = True
            duplicates = [id_val for id_val in set(valid_current_ids) if valid_current_ids.count(id_val) > 1]
            corrections.append(f"æ£€æµ‹åˆ°é‡å¤çš„ id: {duplicates}")
      # å¦‚æœéœ€è¦é‡æ–°æ’åºï¼Œè‡ªåŠ¨ä¿®å¤
    if needs_reorder:
        id_issues = []
        
        # åˆ†æå…·ä½“é—®é¢˜
        if missing_ids or invalid_ids:
            if missing_ids:
                id_issues.append("å­˜åœ¨ç¼ºå°‘idçš„ä½ç½®")
            if invalid_ids:
                id_issues.append("å­˜åœ¨æ— æ•ˆidå€¼")
        
        if valid_current_ids:
            if min(valid_current_ids) != 1:
                id_issues.append("idä¸æ˜¯ä»1å¼€å§‹")
            
            # æ£€æŸ¥è¿ç»­æ€§
            sorted_valid_ids = sorted(valid_current_ids)
            expected_sorted = list(range(1, len(valid_current_ids) + 1))
            if sorted_valid_ids != expected_sorted:
                id_issues.append("idç¼–å·ä¸è¿ç»­")
        
        # é‡æ–°æŒ‰é¡ºåºåˆ†é… idï¼Œå¹¶å°† id å­—æ®µæ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®
        for idx, pos in enumerate(positions):
            new_id = idx + 1
            # åˆ›å»ºæ–°çš„æœ‰åºå­—å…¸ï¼Œid æ”¾åœ¨ç¬¬ä¸€ä¸ª
            new_pos = {"id": new_id}
            # æ·»åŠ å…¶ä»–å­—æ®µ
            for key, value in pos.items():
                if key != "id":
                    new_pos[key] = value
            # æ›´æ–°åŸä½ç½®
            pos.clear()
            pos.update(new_pos)
        
        if id_issues:
            corrections.append(f"idç¼–å·å·²é‡æ–°æ’åºå¹¶ç½®äºé¦–ä½ (é—®é¢˜: {', '.join(id_issues)})")
        else:
            corrections.append("idç¼–å·å·²æŒ‰é¡ºåºé‡æ–°åˆ†é…å¹¶ç½®äºé¦–ä½")
    
    return validation_issues, corrections

# ==================== éªŒè¯ä¿®å¤æ–‡ä»¶ç¼–ç  ====================

def detect_encoding(file_path, read_size=2048):
    try:
        with open(file_path, 'rb') as f:
            raw = f.read(read_size)
            result = chardet.detect(raw)
            return result['encoding'], result['confidence']
    except:
        return None, 0

def fix_encoding_name(enc, file_path=None):
    if not enc:
        return None
    enc = enc.lower()
    if enc in ['ascii']:
        try:
            with open(file_path, 'rb') as f:
                raw = f.read()
                raw.decode('utf-8')
            return 'utf-8'
        except:
            return 'gb18030'
    if enc in ['gb2312', 'gbk', 'windows-1252', 'iso-8859-1', 'gb18030']:
        return 'gb18030'
    return enc

def convert_to_utf8(file_path, original_encoding):
    try:
        encoding = fix_encoding_name(original_encoding, file_path)

        with open(file_path, 'r', encoding=encoding, errors='replace') as f:
            content = f.read()
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"[âœ”] Converted to UTF-8: {file_path} (from {original_encoding} â†’ {encoding})")
    except Exception as e:
        print(f"[âœ–] Failed to convert: {file_path} | Error: {e}")

def process_file(file_path, target_extensions=None):
    if target_extensions and not any(file_path.lower().endswith(ext) for ext in target_extensions):
        return
    encoding, confidence = detect_encoding(file_path)
    if encoding is None or confidence < 0.7:
        print(f"[âš ï¸] Unknown encoding: {file_path} | Detected: {encoding}, Conf: {confidence:.2f}")
        return
    if encoding.lower() == 'utf-8':
        return  # Skip already UTF-8
    convert_to_utf8(file_path, encoding)

def scan_and_convert(path, target_extensions=None):
    if os.path.isfile(path):
        process_file(path, target_extensions)
    elif os.path.isdir(path):
        for dirpath, _, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                process_file(filepath, target_extensions)
    else:
        print(f"âŒ Path not found: {path}")

# ==================== éªŒè¯ä¿®å¤ä½œè€…ä¿¡æ¯ ====================

def process_json_authors(input_path, verbose=False):
    """
    å¤„ç† JSON æ–‡ä»¶ä¸­çš„ä½œè€…ä¿¡æ¯ï¼ˆæ”¯æŒ author â†’ authors ç»“æ„åŒ–è¿ç§»ã€ä½œè€…åé‡å‘½åå’Œé“¾æ¥ç»Ÿä¸€ï¼‰
    
    å‚æ•°ï¼š
        input_path (str): è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤åœ¨è„šæœ¬åŒçº§ï¼‰
        verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—ä¿¡æ¯
        
    è¿”å›ï¼š
        dict: åŒ…å«å¤„ç†æ€»æ•°å’Œä¿®æ”¹æ•°é‡çš„ç»Ÿè®¡ä¿¡æ¯
    """
    result = {
        "total_files": 0,
        "modified_files": 0,
        "errors": []
    }

    # è·å–é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå’Œè„šæœ¬åœ¨åŒä¸€ç›®å½•ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, "author_config.json")

    if not os.path.exists(input_path):
        raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨ï¼š{input_path}")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}")

    # åŠ è½½é…ç½®
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
    except Exception as e:
        raise RuntimeError(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼š{e}")

    author_rename = config.get("rename", {})
    author_links = config.get("links", {})

    # æ„å»ºå¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨
    file_list = []
    if os.path.isfile(input_path) and input_path.endswith(".json"):
        file_list.append(input_path)
    elif os.path.isdir(input_path):
        for root, dirs, files in os.walk(input_path):
            for filename in files:
                if filename.endswith(".json"):
                    file_list.append(os.path.join(root, filename))
    else:
        raise ValueError("è¾“å…¥è·¯å¾„å¿…é¡»æ˜¯ .json æ–‡ä»¶æˆ–ç›®å½•")

    for file_path in file_list:
        result["total_files"] += 1
        if verbose:
            print(f"\nğŸ” å¤„ç†æ–‡ä»¶ï¼š{file_path}")

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            msg = f"âŒ è§£æå¤±è´¥ï¼š{e}"
            if verbose:
                print(msg)
            result["errors"].append((file_path, str(e)))
            continue

        info = data.get("info")
        if not isinstance(info, dict):
            if verbose:
                print("âš ï¸ ç¼ºå°‘ info å­—æ®µ")
            continue

        modified = False
        author_field = info.get("author")

        if author_field is not None:
            if isinstance(author_field, str):
                names = [name.strip() for name in author_field.split("&")]
                new_authors = []
                for name in names:
                    new_name = author_rename.get(name, name)
                    author_obj = {"name": new_name}
                    if new_name in author_links:
                        author_obj["links"] = author_links[new_name]
                    new_authors.append(author_obj)
                data["info"]["authors"] = new_authors
                modified = True
                if verbose:
                    print("âœ… æ›¿æ¢ä¸ºç»“æ„åŒ– authors")

            elif isinstance(author_field, list):
                for author_obj in author_field:
                    if not isinstance(author_obj, dict):
                        continue
                    name = author_obj.get("name")
                    if not name:
                        continue
                    new_name = author_rename.get(name, name)
                    if name != new_name:
                        author_obj["name"] = new_name
                        modified = True
                        if verbose:
                            print(f"ğŸ“ é‡å‘½åï¼š{name} â†’ {new_name}")

                    existing_link = author_obj.pop("link", None) or author_obj.pop("url", None) or author_obj.get("links")
                    if new_name in author_links:
                        if author_obj.get("links") != author_links[new_name]:
                            author_obj["links"] = author_links[new_name]
                            modified = True
                            if verbose:
                                print(f"ğŸ”§ æ›´æ–°é“¾æ¥ï¼š{new_name} â†’ {author_links[new_name]}")
                    elif "links" not in author_obj and existing_link:
                        author_obj["links"] = existing_link
                        modified = True
                        if verbose:
                            print(f"ğŸ”„ æ ‡å‡†åŒ–å·²æœ‰é“¾æ¥å­—æ®µä¸º links â†’ {existing_link}")

        else:
            authors_field = info.get("authors")
            if isinstance(authors_field, list):
                for author_obj in authors_field:
                    if not isinstance(author_obj, dict):
                        continue
                    name = author_obj.get("name")
                    if not name:
                        continue
                    new_name = author_rename.get(name, name)
                    if name != new_name:
                        author_obj["name"] = new_name
                        modified = True
                        if verbose:
                            print(f"ğŸ“ é‡å‘½åï¼ˆauthorsï¼‰ï¼š{name} â†’ {new_name}")

                    existing_link = author_obj.pop("link", None) or author_obj.pop("url", None) or author_obj.get("links")
                    if new_name in author_links:
                        if author_obj.get("links") != author_links[new_name]:
                            author_obj["links"] = author_links[new_name]
                            modified = True
                            if verbose:
                                print(f"ğŸ”§ æ›´æ–°é“¾æ¥ï¼ˆauthorsï¼‰ï¼š{new_name} â†’ {author_links[new_name]}")
                    elif "links" not in author_obj and existing_link:
                        author_obj["links"] = existing_link
                        modified = True
                        if verbose:
                            print(f"ğŸ”„ æ ‡å‡†åŒ–å·²æœ‰é“¾æ¥å­—æ®µä¸º links â†’ {existing_link}")
            else:
                # if verbose:
                    print("âš ï¸ ç¼ºå°‘ author å­—æ®µï¼Œä¸” authors éæ ‡å‡†æ ¼å¼")

        if modified:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            result["modified_files"] += 1
            if verbose:
                print("âœ… å†™å…¥å®Œæˆ")
        else:
            if verbose:
                print("â­ï¸ æ— éœ€ä¿®æ”¹")

    if verbose:
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼šå…± {result['total_files']} ä¸ª JSON æ–‡ä»¶ï¼Œä¿®æ”¹äº† {result['modified_files']} ä¸ª")

# ==================== ç›®å½•ç»“æ„æ ¡éªŒ ====================

def validate_directory_structure(dir_path, parent_folders=None):
    """æ ¡éªŒç›®å½•ç»“æ„ï¼Œæ£€æµ‹JSONæ–‡ä»¶å’Œç›®å½•åŒçº§çš„æƒ…å†µ"""
    if parent_folders is None:
        parent_folders = []
    
    errors = []
    
    try:
        items = os.listdir(dir_path)
        files = []
        directories = []
        
        # åˆ†ç±»æ–‡ä»¶å’Œç›®å½•
        for item in items:
            item_path = os.path.join(dir_path, item)
            if os.path.isfile(item_path):
                files.append(item)
            elif os.path.isdir(item_path):
                directories.append(item)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰JSONæ–‡ä»¶å’Œç›®å½•åŒçº§
        json_files = [f for f in files if f.lower().endswith('.json')]
        
        if json_files and directories:
            relative_path = '/'.join(parent_folders + [os.path.basename(dir_path)]) if parent_folders else os.path.basename(dir_path)
            error_msg = f"âŒ ç›®å½•ç»“æ„é”™è¯¯: åœ¨ç›®å½• \"{relative_path}\" ä¸­å‘ç°JSONæ–‡ä»¶å’Œå­ç›®å½•åŒçº§å­˜åœ¨ã€‚JSONæ–‡ä»¶: {json_files}, å­ç›®å½•: {directories}"
            errors.append(error_msg)
            print(error_msg)
        
        # é€’å½’æ£€æŸ¥å­ç›®å½•
        for dir_name in directories:
            sub_dir_path = os.path.join(dir_path, dir_name)
            sub_errors = validate_directory_structure(sub_dir_path, parent_folders + [os.path.basename(dir_path)])
            errors.extend(sub_errors)
        
    except Exception as error:
        relative_path = '/'.join(parent_folders + [os.path.basename(dir_path)]) if parent_folders else os.path.basename(dir_path)
        error_msg = f"âŒ æ— æ³•è®¿é—®ç›®å½• \"{relative_path}\": {str(error)}"
        errors.append(error_msg)
        print(error_msg)
    
    return errors

# ==================== ä¸»éªŒè¯é€»è¾‘ ====================

def initialize_data(data, file_path):
    """åˆå§‹åŒ–æ•°æ®ç»“æ„ï¼Œç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨"""
    messages = []

    if "info" not in data:
        data["info"] = {}
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ info å­—æ®µï¼Œå·²æ·»åŠ é»˜è®¤å€¼")

    info = data["info"]
    filename = os.path.splitext(os.path.basename(file_path))[0]

    # æ£€æŸ¥å¹¶æ·»åŠ å¿…è¦çš„å­—æ®µ
    if "name" not in info:
        info["name"] = filename
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ name å­—æ®µï¼Œå·²è®¾ç½®ä¸ºæ–‡ä»¶å: {info['name']}")

    if "type" not in info:
        info["type"] = "collect"
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ type å­—æ®µï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤å€¼: collect")

    if "authors" not in info:
        author_name = os.getenv("GITHUB_ACTOR", "æœªçŸ¥ä½œè€…")
        author_link = "https://github.com/" + os.getenv("GITHUB_ACTOR", "babalae/bettergi-scripts-list")
        info["authors"] = [{"name": author_name, "links": author_link}]
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ authors å­—æ®µï¼Œå·²è®¾ç½®ä¸º: {info['authors']}")

    if "version" not in info:
        info["version"] = DEFAULT_VERSION
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ version å­—æ®µï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤å€¼: {DEFAULT_VERSION}")

    if "bgi_version" not in info:
        info["bgi_version"] = DEFAULT_BGI_VERSION
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ bgi_version å­—æ®µï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤å€¼: {DEFAULT_BGI_VERSION}")

    if "positions" not in data:
        data["positions"] = []
        messages.append(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ positions å­—æ®µï¼Œå·²æ·»åŠ ç©ºæ•°ç»„")

    return data

def check_actions_compatibility(positions, bgi_version):
    """æ£€æŸ¥æ‰€æœ‰ä½ç½®çš„ action å…¼å®¹æ€§"""
    compatibility_issues = []
    validation_issues = []

    for idx, pos in enumerate(positions):
        action_type = pos.get("action", "")
        action_params = pos.get("params", "")

        if action_type:
            compat_issues, valid_issues = check_action_compatibility(action_type, action_params, bgi_version)

            for issue in compat_issues:
                compatibility_issues.append(f"ä½ç½® {idx+1}: {issue}")

            for issue in valid_issues:
                validation_issues.append(f"ä½ç½® {idx+1}: {issue}")

    return compatibility_issues, validation_issues

def update_bgi_version_for_compatibility(info, compatibility_issues, auto_fix):
    """æ ¹æ®å…¼å®¹æ€§é—®é¢˜æ›´æ–° BGI ç‰ˆæœ¬"""
    corrections = []

    if auto_fix and compatibility_issues:
        max_required = extract_required_version(compatibility_issues)

        if max_required:
            # ç¡®ä¿ max_required æ²¡æœ‰ v å‰ç¼€
            max_required = max_required.lstrip('v')

            try:
                current_bgi = parse_bgi_version(info["bgi_version"])
                if current_bgi and current_bgi < VersionInfo.parse(max_required):
                    info["bgi_version"] = max_required
                    corrections.append(f"bgi_version {info['bgi_version']} è‡ªåŠ¨æ›´æ–°ä¸º {max_required} ä»¥å…¼å®¹æ‰€æœ‰åŠŸèƒ½")
                    return [], corrections
            except ValueError as e:
                # print(f"è­¦å‘Š: ç‰ˆæœ¬å·è§£æå¤±è´¥ - {e}")
                info["bgi_version"] = DEFAULT_BGI_VERSION
                corrections.append(f"bgi_version è‡ªåŠ¨æ›´æ–°ä¸º {DEFAULT_BGI_VERSION} (ç‰ˆæœ¬è§£æå¤±è´¥)")
                return [], corrections

    return compatibility_issues, corrections

def validate_file(file_path, auto_fix=False):
    """éªŒè¯å¹¶ä¿®å¤ JSON æ–‡ä»¶"""
    # åŠ è½½æ–‡ä»¶
    data, error = load_json_file(file_path)
    if error:
        print(error)
        return [error]

    # è·å–åŸå§‹æ–‡ä»¶
    original_data, source = get_original_file(file_path) if auto_fix else (None, None)
    is_new = not original_data if auto_fix else True

    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    data = initialize_data(data, file_path)
    info = data["info"]
    filename = os.path.splitext(os.path.basename(file_path))[0]

    # æ”¶é›†æ‰€æœ‰ä¿®æ­£ - ä¿®å¤ï¼šæ·»åŠ äº†è¿™ä¸€è¡Œæ¥å®šä¹‰ all_corrections å˜é‡
    all_corrections = []

    # æ£€æŸ¥å¿…è¦å­—æ®µ
    corrections = ensure_required_fields(info, filename)
    all_corrections.extend(corrections)

    # å¤„ç†åæ ‡
    coord_changed = process_coordinates(data["positions"])
    if coord_changed:
        all_corrections.append("åæ ‡å€¼è‡ªåŠ¨ä¿ç•™å››ä½å°æ•°")

    # æ£€æŸ¥ BGI ç‰ˆæœ¬å…¼å®¹æ€§
    bgi_version, corrections = check_bgi_version_compatibility(info["bgi_version"], auto_fix)
    if corrections:
        info["bgi_version"] = bgi_version
        all_corrections.extend(corrections)    # æ£€æŸ¥ä½ç½®å­—æ®µ - ä¿®æ”¹ä¸ºæ¥æ”¶ä¸‰ä¸ªè¿”å›å€¼
    position_issues, notices, pos_corrections = check_position_fields(data["positions"])
    if auto_fix and pos_corrections:
        all_corrections.extend(pos_corrections)

    # æ£€æŸ¥ä½ç½® ID ç¼–å·
    if auto_fix:
        id_validation_issues, id_corrections = check_position_ids(data["positions"])
        if id_corrections:
            all_corrections.extend(id_corrections)
        position_issues.extend(id_validation_issues)

    # æ£€æŸ¥ action å…¼å®¹æ€§
    compatibility_issues, action_validation_issues = check_actions_compatibility(data["positions"], info["bgi_version"])
    position_issues.extend(action_validation_issues)

    # æ ¹æ®å…¼å®¹æ€§é—®é¢˜æ›´æ–° BGI ç‰ˆæœ¬
    compatibility_issues, corrections = update_bgi_version_for_compatibility(info, compatibility_issues, auto_fix)
    all_corrections.extend(corrections)

    # æ›´æ–°ç‰ˆæœ¬å· - åªæœ‰ä»ä¸Šæ¸¸ä»“åº“è·å–çš„æ–‡ä»¶æ‰æ›´æ–°ç‰ˆæœ¬å·
    # if auto_fix:
    if False:
        has_original_version = False
        original_version = None

        if original_data and "info" in original_data and "version" in original_data["info"]:
            original_version = original_data["info"]["version"]
            has_original_version = True
            print(f"æˆåŠŸè·å–åŸå§‹ç‰ˆæœ¬å·: {original_version}")
        else:
            print("æœªæ‰¾åˆ°åŸå§‹ç‰ˆæœ¬å·ï¼Œå°†è§†ä¸ºæ–°æ–‡ä»¶å¤„ç†")

        # åªæœ‰åœ¨æ²¡æœ‰åŸå§‹ç‰ˆæœ¬å·æ—¶æ‰è§†ä¸ºæ–°æ–‡ä»¶
        is_new = not has_original_version

        print(f"åŸå§‹ç‰ˆæœ¬å·: {original_version}, å½“å‰ç‰ˆæœ¬å·: {info['version']}, æ˜¯å¦æ–°æ–‡ä»¶: {is_new}, æ¥æº: {source}")

        # åªæœ‰å½“æ–‡ä»¶æ¥æºæ˜¯ä¸Šæ¸¸ä»“åº“æ—¶æ‰æ›´æ–°ç‰ˆæœ¬å·
        if source == "upstream":
            new_version = process_version(info["version"], original_version, is_new)
            if new_version != info["version"]:
                info["version"] = new_version
                all_corrections.append(f"version è‡ªåŠ¨æ›´æ–°ä¸º {new_version}")
                print(f"ç‰ˆæœ¬å·å·²æ›´æ–°: {info['version']}")
            else:
                print(f"ç‰ˆæœ¬å·æœªå˜åŒ–: {info['version']}")
        else:
            print(f"è¿™æ˜¯PRæäº¤çš„æ–‡ä»¶ï¼Œä¿æŒç‰ˆæœ¬å·ä¸å˜: {info['version']}ï¼ˆåˆå¹¶åå†æ›´æ–°ç‰ˆæœ¬ï¼‰")

    # åˆå¹¶æ‰€æœ‰é€šçŸ¥
    for issue in compatibility_issues:
        notices.append(issue)

    for issue in position_issues:
        notices.append(issue)

    # ä¿å­˜ä¿®æ­£
    if auto_fix:
        if all_corrections or position_issues:
            if save_json_file(file_path, data):
                print("âœ… æ–‡ä»¶å·²ä¿å­˜")
            else:
                notices.append("âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥")

    return notices

def main():
    import argparse

    parser = argparse.ArgumentParser(description='æ ¡éªŒ BetterGI è„šæœ¬æ–‡ä»¶')
    parser.add_argument('path', help='è¦æ ¡éªŒçš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--fix', action='store_true', help='è‡ªåŠ¨ä¿®å¤é—®é¢˜')
    parser.add_argument('--structure', action='store_true', help='æµ…è‰çš„æ°¨æ°”æå¾—ä»€ä¹ˆç»“æ„æ ¡éªŒ')
    args = parser.parse_args()

    path = args.path
    auto_fix = args.fix
    structure = args.structure
    all_notices = []  # åˆå§‹åŒ– all_notices å˜é‡

    # é¦–å…ˆæ‰§è¡Œç›®å½•ç»“æ„æ ¡éªŒ
    if structure:
        if os.path.isdir(path):
            print("ğŸ” å¼€å§‹ç›®å½•ç»“æ„æ ¡éªŒ...")
            structure_errors = validate_directory_structure(path)
            if structure_errors:
                print("\nâŒ ç›®å½•ç»“æ„æ ¡éªŒå¤±è´¥ï¼Œå‘ç°ä»¥ä¸‹é”™è¯¯:")
                for error in structure_errors:
                    print(f"- {error}")
                print("\nè¯·ä¿®å¤ä¸Šè¿°ç›®å½•ç»“æ„é—®é¢˜åé‡æ–°æäº¤ã€‚")
                print("\nç›®å½•ç»“æ„è§„èŒƒè¯´æ˜:")
                print("- ä¸å…è®¸JSONæ–‡ä»¶å’Œå­ç›®å½•åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹å…±å­˜")
                print("- å»ºè®®å°†JSONæ–‡ä»¶ç§»åŠ¨åˆ°ä¸“é—¨çš„å­ç›®å½•ä¸­")
                exit(1)
            print("âœ… ç›®å½•ç»“æ„æ ¡éªŒé€šè¿‡")

    if os.path.isfile(path) and path.endswith('.json'):
        scan_and_convert(path)
        process_json_authors(path)
        # print(f"\nğŸ” æ ¡éªŒæ–‡ä»¶: {path}")
        notices = validate_file(path, auto_fix)
        if notices:
            all_notices.extend([f"{path}: {n}" for n in notices])  # æ·»åŠ åˆ° all_notices
            print("\næ ¡éªŒæ³¨æ„äº‹é¡¹:")
            for notice in notices:
                print(f"- {notice}")
        else:
            print("âœ… æ ¡éªŒå®Œæˆï¼Œæ²¡æœ‰å‘ç°é—®é¢˜")
    elif os.path.isdir(path):
        for root, _, files in os.walk(path):
            for file in files:
                if file.endswith('.json'):
                    file_path = os.path.join(root, file)
                    print(f"\nğŸ” æ ¡éªŒæ–‡ä»¶: {file_path}")
                    scan_and_convert(file_path)
                    process_json_authors(file_path)
                    notices = validate_file(file_path, auto_fix)
                    if notices:
                        all_notices.extend([f"{file_path}: {n}" for n in notices])

        if all_notices:
            print("\næ‰€æœ‰æ ¡éªŒæ³¨æ„äº‹é¡¹:")
            for notice in all_notices:
                print(f"- {notice}")
        else:
            print("\nâœ… æ‰€æœ‰æ–‡ä»¶æ ¡éªŒå®Œæˆï¼Œæ²¡æœ‰å‘ç°é—®é¢˜")
    else:
        print(f"âŒ æ— æ•ˆçš„è·¯å¾„: {path}")

if __name__ == "__main__":
    main()
